{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TeO6LOtwk0OI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, depth, layers_units=[], activation=[], learning_rate=.01):\n",
        "        self.depth = depth\n",
        "        self.layers_units = layers_units\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = []\n",
        "    \n",
        "    # initialize weights\n",
        "    def initialize_weights(self):\n",
        "        # set random seed\n",
        "        torch.manual_seed(0)\n",
        "\n",
        "        self.params = {}\n",
        "        c = 1e-4\n",
        "        input = self.X.shape[1]\n",
        "        for i in range(self.depth):\n",
        "            \"\"\"\n",
        "                input layer is not counted in depth\n",
        "                for each layer initialize W and b and save values in params\n",
        "                N1 -> node-1\n",
        "                w1 -> weight for x1 in the input\n",
        "\n",
        "                      N1-N2-N3 \n",
        "                W = | w1 w1 w1|\n",
        "                    | w2 w2 w2|\n",
        "                    | w3 w3 w3|\n",
        "\n",
        "                hidden layer input = output of previous layers\n",
        "                \n",
        "            \"\"\"\n",
        "            # initialize weights\n",
        "            self.params[f'W{i+1}'] = torch.randn(input, self.layers_units[i],\n",
        "                                                    dtype=torch.float32)\n",
        "            # initialize bias\n",
        "            self.params[f'b{i+1}'] = torch.zeros(self.layers_units[i], )\n",
        "\n",
        "            # update input for next layer to be output of curren tlayer\n",
        "            input = self.layers_units[i]\n",
        "\n",
        "    # activation unit\n",
        "    def activate(self, z, actv):\n",
        "        # sigmoid\n",
        "        if actv == 'sig':\n",
        "            return 1 / ( 1 + torch.exp(-z))\n",
        "        \n",
        "        # relu \n",
        "        if actv == 'relu':\n",
        "            z[z < 0] = 0\n",
        "            return z\n",
        "\n",
        "        # softmax\n",
        "        if actv == 'softmax':\n",
        "            # numerical stability by subtracting constant for z\n",
        "            z = z - torch.max(z, dim =1, keepdim=True)[0]\n",
        "            z = torch.exp(z) / (torch.sum(torch.exp(z), dim=1, keepdim=True))\n",
        "            return z\n",
        "\n",
        "    # activation grad\n",
        "    def actv_grad(self, G, actv):\n",
        "        # gradient w.r.t sigmoid function\n",
        "        if actv == 'sig':\n",
        "            return self.activate(G, actv) * (1 - self.activate(G, actv))\n",
        "        \n",
        "        # gradient with respect to relu\n",
        "        if actv == 'relu':\n",
        "            G[G < 0] = 0\n",
        "            G[G > 0] = 1\n",
        "            return G\n",
        "\n",
        "    # feed forward\n",
        "    def feed_forward(self, x):\n",
        "        # dictionary to cache forward pass results\n",
        "        layer_output = {}\n",
        "        \n",
        "        # set x to be A0\n",
        "        layer_output['A0'] = x\n",
        "\n",
        "        # feed x forward in the netword\n",
        "        for l in range(self.depth):\n",
        "            # grab layer params W and b\n",
        "            W = self.params[f'W{l+1}']\n",
        "            b = self.params[f'b{l+1}']\n",
        "\n",
        "            # grab layer activation\n",
        "            actv = self.activation[l]\n",
        "\n",
        "            # calculate Layer output\n",
        "            Z = torch.mm(x, W) + b\n",
        "            A = self.activate(Z, actv)\n",
        "\n",
        "            # layer output is the input to next layer\n",
        "            x = A\n",
        "\n",
        "            # cache layer results\n",
        "            layer_output[f'Z{l+1}'] = Z\n",
        "            layer_output[f'A{l+1}'] = A\n",
        "\n",
        "        return layer_output\n",
        "\n",
        "    # back propagate\n",
        "    def backprop(self, x, Y, layer_output, reg=0.0):\n",
        "        # last layer output is the score\n",
        "        scores = layer_output[f'A{self.depth}']\n",
        "\n",
        "        N = x.shape[0]\n",
        "        # compute softmax loss\n",
        "        idx0 = torch.arange(N).reshape(-1, 1)\n",
        "        loss = torch.sum(-torch.log(scores[[idx0, Y]]))/N\n",
        "\n",
        "        # modify convert y from index of correct class to matrix \n",
        "        \"\"\"\n",
        "            Y -> 1D tensor with values equal to index of correct class\n",
        "        \"\"\"\n",
        "        Y_ = torch.zeros(N, self.c)\n",
        "        Y_[idx0, Y] = 1\n",
        "\n",
        "        # dictionary to cache grads\n",
        "        grads = {}\n",
        "\n",
        "        # gradient w.r.t classification\n",
        "        dCE = None\n",
        "\n",
        "        # backprop Error through network layers\n",
        "        for i in range(self.depth, 0, -1):\n",
        "            # grab layer activation function\n",
        "            layer_actv = self.activation[i-1]\n",
        "\n",
        "            # if output layer \n",
        "            if i == self.depth:\n",
        "                dCE = (scores - Y_)/N\n",
        "            \n",
        "            else:\n",
        "                # grab next layer weight matrix\n",
        "                W = self.params[f'W{i+1}']\n",
        "                # grab layer input \n",
        "                Z = layer_output[f'Z{i}']\n",
        "                # backpropagate gradient from next layer to current layer\n",
        "                dCE = torch.mm(dCE, W.t()) * self.actv_grad(Z, layer_actv) \n",
        "\n",
        "            # calculate gradient of current layer parameters\n",
        "            A = layer_output[f'A{i-1}']\n",
        "            grads[f'dW{i}'] = torch.mm(A.T, dCE)\n",
        "            grads[f'db{i}'] = torch.sum(dCE)\n",
        "            \n",
        "        return scores, grads, loss\n",
        "\n",
        "    # update weights \n",
        "    def update_weights(self, grads):\n",
        "        for i in range(self.depth):\n",
        "            self.params[f'W{i+1}'] = self.params[f'W{i+1}'] - grads[f'dW{i+1}'] * self.learning_rate\n",
        "            self.params[f'b{i+1}'] = self.params[f'b{i+1}'] - grads[f'db{i+1}'] * self.learning_rate\n",
        "\n",
        "    # train network\n",
        "    def train(self, X, Y, c, batch, num_iter):\n",
        "        self.c = c\n",
        "        self.X = X\n",
        "        # initialize weights\n",
        "        self.initialize_weights()\n",
        "\n",
        "        # train\n",
        "        for i in range(num_iter):\n",
        "            for b in range(0, X.shape[0], batch):\n",
        "                # grab batch of the data\n",
        "                x = X[b:b+batch, :]\n",
        "                y = Y[b:b+batch, :]\n",
        "\n",
        "                # feed forward\n",
        "                layer_output = self.feed_forward(x)\n",
        "\n",
        "                # backprop\n",
        "                scores, grads, loss = self.backprop(x, y, layer_output)\n",
        "                self.loss.append({\n",
        "                    f'iteration:{i}-batch:{b//batch}': {'loss': loss}\n",
        "                })\n",
        "\n",
        "                # update weights\n",
        "                self.update_weights(grads)\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Epoch {i}\", f\"Loss {loss}\")\n",
        "\n",
        "    # make predictions\n",
        "    def predict(self, X):\n",
        "        layer_output = self.feed_forward(X)\n",
        "        pred = layer_output[f'A{self.depth}']\n",
        "        y_ = torch.max(pred, axis=1, keepdim=True)[1]\n",
        "        return y_"
      ],
      "metadata": {
        "id": "MQq-joX-lM0M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([\n",
        "        [ 25.4410,  -7.1635,  -4.9337,   1.2671],\n",
        "        [  1.0136,  -4.0353,   9.0226,   8.0993],\n",
        "        [ -6.8838,   1.3724,  10.3774,   0.9255],\n",
        "        [ -3.7518,  -0.9082,  20.6391, -18.1638],\n",
        "        [ -2.7188,   2.8113, -10.3986,   7.7653]\n",
        "    ])\n",
        "\n",
        "Y = torch.tensor([[0],\n",
        "                  [1],\n",
        "                  [2],\n",
        "                  [2],\n",
        "                  [1]])"
      ],
      "metadata": {
        "id": "RMNEUP9VPuZS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = NeuralNetwork(depth=2, layers_units=[5, 3], activation=['relu', 'softmax'])"
      ],
      "metadata": {
        "id": "SEaMCPJ3qfcw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj.train(X, Y, c=3, batch=4, num_iter=100)"
      ],
      "metadata": {
        "id": "BuSnQWlpUEAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95a1970-99df-4004-892b-f0041f8dd5c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss 1.0751113891601562\n",
            "Epoch 10 Loss 0.013420832343399525\n",
            "Epoch 20 Loss 0.0077223158441483974\n",
            "Epoch 30 Loss 0.0053812554106116295\n",
            "Epoch 40 Loss 0.0041109067387878895\n",
            "Epoch 50 Loss 0.003315814072266221\n",
            "Epoch 60 Loss 0.0027725351974368095\n",
            "Epoch 70 Loss 0.002378369215875864\n",
            "Epoch 80 Loss 0.00207980046980083\n",
            "Epoch 90 Loss 0.0018460494466125965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqXclsTogn3w",
        "outputId": "92e5237d-eb93-4bd7-edf7-9f3bb8c0f4a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [1],\n",
              "        [2],\n",
              "        [2],\n",
              "        [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}